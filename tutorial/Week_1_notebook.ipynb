{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: design and data representation\n",
    "This week's tutorial/assignment will be about the basics of pattern analysis. It's very important that you understand the discussed concepts, as they form the basis of the rest of the course's tutorials/assignments. Additionally, we'll discuss some new programming concepts (mostly object-oriented programming), as this will become more and more important as you're going to learn how to program everything yourself in Python. \n",
    "\n",
    "Specifically, at the end of this tutorial, you will be able to:\n",
    "\n",
    "* Understand the major differences between within- and between-subject first-level designs for pattern estimation;\n",
    "* Understand the concept of object-oriented programming and know how to construct classes for neuroimaging-purposes;\n",
    "* Implement functionality to load, organize, and efficiently represent voxel pattern for pattern analyses;\n",
    "* Implement simple preprocessing functions/methods for voxel patterns;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Designs for pattern analyses\n",
    "There are many ways in which you can categorize different types of pattern analyses (a topic which you'll work on in Thursdays's seminar), but one of the most basic categorizations is in terms of whether analyses are **within-subject** or **between-subject**. The major distinction revolves around whether you want to investigate an (experimental) factor that varies or is manipulated within subjects or that varies across subjects (i.e. individual differences or experimental between-subject designs).\n",
    "\n",
    "Importantly, both types of analyses differ in what is regarded as an *instance of a pattern*:\n",
    "- in within-subject analyses, *each instance of your feature-of-interest represents one pattern*;\n",
    "- in between-subject analyses, *each subject represents one pattern*.\n",
    "\n",
    "In part 3 of this tutorial (Data representation), we'll discuss how these patterns in within- and between-subject analyses are typically represented. This part (Designs for pattern analyses) will focus on how to extract patterns from fMRI data.\n",
    "\n",
    "As explained in the lecture, there are three ways to extract patterns from fMRI data: \n",
    "\n",
    "1. extract activity at a certain timepoint (e.g. 3 TRs after stimulus presentation);\n",
    "2. extract the *average* activity within a time-range (e.g. 2-4 TRs after stimulus presentation);\n",
    "3. extract patterns of $\\beta$-estimates by fitting a HRF per trial using the GLM you're familiar with; \n",
    "\n",
    "In the first part of this tutorial, we'll focus on method 3 (as this has been shown to yield the most stable pattern estimates). We will show you how the design matrices for the GLM in method 3 look like for both (single-trial) within-subject analyses and between-subject analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Within-subject designs\n",
    "Often, \"trials\" (i.e. instances of your feature-of-interest) in within-subject designs are modelled as separate regressors in a first-level analysis. In other words, you model each trial as a separate (*single!*) HRF-response. Below, we included an image of a single-trial design (of the hypothetical faces vs. houses experiment) as created in FSL:<img src=\"single_trial_design.png\" alt=\"Drawing\" heigth=\"100\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each trial (either a face or a house) gets it's own regressor. Then, as depicted below the design matrix, a contrast-against-baseline is created for each regressor (trial). After you run a first-level analysis using this design, you'll have whole-brain maps containing statistics values (beta-values, t-values, or z-values) for each trial that represent the trial's estimated (whole-brain) pattern. Usually, t- or z-values are used instead of betas.  \n",
    "\n",
    "**Importantly, this design thus specifies that each \"trial\" represents a sample with its own pattern (voxels).**\n",
    "\n",
    "Before you go on, make sure you understand this image! This image represents basically all you need to understand about single-trial designs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement your own single-trial design\n",
    "In this section, you'll create your own single-trial matrix corresponding to a (real) working memory experiment (similar to the one described in Monday's lecture). In this experiment, one condition (\"ACTIVE\") subjects had to remember a configuration of bars and after a retention period had to respond whether one of the bars has changed in the test-image or not. In the other condition (\"PASSIVE\") they just watched a blank screen and had to respond with a random answer. The experiment is depicted schematically below:\n",
    "\n",
    "![test](WM_example.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, subjects performed 40 trials, of which 32 were of the \"ACTIVE\" condition and 8 were of the \"PASSIVE\" condition. In the following section, we'll generate a single-trial design that aims to estimate the pattern for each trial.\n",
    "\n",
    "Below, we'll load the onsets (and durations and conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First some imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numpy array with onsets (column 1), durations (column 2), and conditions (column 3)\n",
    "# N.B.: condition 0 = passive, condition 1 = active\n",
    "onsets = np.loadtxt('onsets.csv').astype(int)\n",
    "print(\"Onset Duration Condition\")\n",
    "print(onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Practice your numpy-skills! How would you calculate how many active-trials and how many passive-trials there were? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the onsets (and duration) are here defined in seconds (not TRs). Let's assume that the fMRI-run has a TR of 2. Now, we can convert (very easily!) the onsets/durations-in-seconds to onsets/durations-in-TRs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Convert the onsets and durations from seconds to TRs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ToDo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you might remember, for each regressor we need to create a regressor of zeros and ones, in which the ones represent the moments in which the particular trial was presented. Remember, if a stimulus lasted 6 seconds (i.e. 3 TRs), make sure that your regressor also models your event for this duration! \n",
    "\n",
    "So, for example, if you have a (hypothetical) run with a duration of 15 TRs, and you show a stimulus at TR=3 for the duration of 3 TRs (i.e. 6 seconds), then you'd code your regressor as:\n",
    "\n",
    "`[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Below, we initialized a stimulus vector (`stim_vec`) of shape=[162, 40], i.e. timepoints x trials (this run was 162 TRs long), with zeros. Each of the 40 rows represents one trial. Loop over the colums of the `stim_vec` matrix and fill the times at onset till the onset + 2 TRs with ones. Remember, the first index in Python is zero (not 1!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill the stim_vec variable with ones at the indices of the onsets per trial!\n",
    "stim_vec = np.zeros((162, 40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to convolve an HRF with the stimulus-vectors and we'll have a complete single-trial design! Don't worry, we do this for you. We'll also plot it to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functions import double_gamma\n",
    "\n",
    "hrf = double_gamma(range(162))\n",
    "\n",
    "# List comprehension (fancy for-loop) + stack results back to a matrix\n",
    "X = np.vstack([np.convolve(hrf, stim_vec[:, i], 'full')[:162] for i in range(40)]).T\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for plot in range(40):\n",
    "    plt.subplot(1, 40, plot+1)\n",
    "    plt.plot(X[:, plot], range(X.shape[0])[::-1])\n",
    "    plt.axis('off')\n",
    "    plt.text(-2, 170, 'Active' if onsets[plot, 2] else 'Passive', rotation=45)\n",
    "\n",
    "plt.text(-60, -10, 'Regressors (trials)', ha='center', fontsize=20)\n",
    "plt.text(-120, 80, 'Time (TR)', va='center', rotation='vertical', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the lecture, we can use this design to extract patterns of $\\beta$-values for each regressor. But [it has been shown](http://www.sciencedirect.com/science/article/pii/S1053811910007834) that converting $\\beta$-values to t-values often creates more stable and robust patterns. As you might remember from \"Neuroimaging: fMRI\", $\\beta$-values can be normalized to t-values by defining a contrast-against-baseline that is subsequently used in the formula for the t-value. \n",
    "\n",
    "Suppose I want to convert the patterns of $\\beta$-values *for each trial* in the design above to t-values, how would my contrast-matrix look like? (Hint: check out the first image of this notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: create a design matrix in which each row represents the contrast-against-baseline-vector of a single trial of the within-subject design above. (Hint: check out the `numpy.eye()` function)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do your ToDo here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you know how to create a \"single-trial design\" for pattern analyses! As a short summary:\n",
    "- Model each \"trial\" (or more generally, \"instance\") as a separate HRF-convolved regressor;\n",
    "- Additionally define a contrast-matrix in which each trial is contrasted against baseline in order to create t-value/z-value patterns;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: We actually ran the first-level analysis of the single-trial working-memory design outlined above for one subject (sub-0037) using FSL. You can find the results in the directory: `week_1/sub-0037_workingmemory_WITHIN.feat`. Check out the `stats` subdirectory. You will see a number of different types of niftis (pe, cope, varcope, zstat, tstat). Do you remember what each type refers to? (Hint: check out [this link](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FEAT/UserGuide#FEAT_Output).) Also, notice how there are 40 different files of each type, corresponding to the 40 regressors in our single-trial design! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: in the single-trial designs you've seen so far, we *only* modelled the trials themselves. Now, however, imagine the following experiment: subjects are shown a series of images; an image can be either of condition \"A\" or of condition \"B\". Each image is preceded by a cue, which is important to keep the participant focused, but is further irrelevant for the experiment. Suppose you're interested in doing a pattern analysis to show that the patterns of condition \"A\" images are different than patterns of condition \"B\" images in superior temporal cortex. Would you include the (onsets of the) cues in the design? If so, why (and how)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, you now know how a single-trial (within-subject) design looks like and what it produces (i.e. single-trial pattern estimates in the form of whole-brain beta/t-stat/z-stat maps). Before we go on to between-subject designs, we are going to load the patterns from one *sample* (in within-subject analyses: one *trial*) in fslview. This will hopefully give you some more \"intuition\" on what is meant with a single trial pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: open up a new terminal or terminal-tab and start fslview (by typing `fslview` in the terminal). Now, click \"File\" > \"Open\", navigate to `week_1/sub-0037_workingmemory_WITHIN.feat/stats` and select `tstat1.nii.gz`. Now, to visualize the patterns somewhat more intuitively, set in the fslview header \"Min\" to 0 and \"Max\" to 5. Then, click the little circle with the blue \"i\": <img src=\"viz.png\">\n",
    "<br>\n",
    "Under the \"lookup table options\", select the \"Red-Yellow\" colormap. What you visualize here are all the voxels that activate during this particular trial. However, a pattern is not necessarily only the voxels that *activate*, but also those that *deactivate*. A pattern is *any* estimated response of the brain during an instance of a sample (here: a trial). Therefore, let's also visualize the deactivating voxels. To do so, click \"File\" > \"Add\" and select the `tstat1.nii.gz` image *again*, but now set \"Min\" to 0 and \"Max\" to -5. For this file, select the colormap \"Blue-Lightblue\". \n",
    "\n",
    "What you should see now is something like this: <img src=\"screenshot_within.png\">\n",
    "\n",
    "<br>\n",
    "What we visualized here is an example of a within-subject pattern of a particular trial (\"sample\"), in which the activated voxels (relative to baseline) are colored red/yellow, and the deactivated voxels (relative to baseline) are colored blue/lightblue. Make sure you understand how this image represents the pattern of a single sample.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Between-subject designs\n",
    "Between-subject designs are way more 'simple' than within-subject designs. Basically, they're the same as traditional 'activation-based' (univariate) designs in which each condition (*not* trial!) gets its own regressor. Thus, the final design-matrix is, in this case of the WM experiment, only of size 162 x 2 (one 'active' regressor, one 'passive' regressor). This yields then only one beta/t-stat/z-stat image (pattern) per subject. Because subjects represent the samples in between-subject analyses, these 'univariate' designs are applied for multiple subject, whose patterns will make up all the samples in our analysis in the end.\n",
    "\n",
    "But to reiterate how such a 'univariate' design looks like again (and how it differs from a single-trial within-subject design), we'll create it ourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Below, we again initialized a stimulus-vector matrix with zeros (of shape TRs \\* conditions). Now fill it again with ones at times of the stimulus-onsets of each condition.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stim_vec_between = np.zeros((162, 2))\n",
    "\n",
    "# Fill it with ones at the onsets of the stimuli using the onsets variable from earlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given that you succesfully filled the stim_vec_between variable with ones, let's plot it again to see how this between-subject design looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xb = np.vstack([np.convolve(hrf, stim_vec_between[:, i], 'full')[:162] for i in range(2)]).T\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "for plot in range(2):\n",
    "    plt.subplot(1, 2, plot+1)\n",
    "    plt.plot(Xb[:, plot], range(Xb.shape[0])[::-1])\n",
    "    plt.text(-2, 180, 'Passive' if plot == 0 else 'Active', rotation=45)\n",
    "    plt.axis('off')\n",
    "plt.text(-2, -15, 'Regressors (conditions)', ha='center', fontsize=10)\n",
    "plt.text(-8, 80, 'Time (TR)', va='center', rotation='vertical', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, you'd evaluate this design using a dedicated neuroimaging software package like FSL. Like the within-subject single trial analysis above, we did this for you already. But, as it pertains to a *between-subject* pattern analysis, we did this for 5 subjects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: check out the directory `week_1/BETWEEN`. This contains 5 feat-directories, one for each subject. Check out one of the subject-directories and look at the `design.png` file. Make sure, again, that you understand the regressors and the contrasts defined. Now, look at the `reg_standard` directory\\*. Make sure you understand why there are (only) three files of each type. What do they correspond to? Lastly, as noted before, in between-subject analyses *each subject represents one instance of a pattern*. In that case, how many instances of patterns (i.e. **samples**) do we have in this between-subject analysis?\n",
    "\n",
    "<br><br>\n",
    "\\* The `reg_standard` directory contains (the relevant) files from the `stats` directory, but in standard (MNI152 2mm) space instead of \"native\" (functional) space. Can you think of why you should use patterns in standard space for between-subject analyses? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize, again, the estimated *between-subject* instance of a pattern for a single subject, let's fire up `fslview` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: click \"File\" > \"Open\" and select `week_1/BETWEEN/pi0060/pi0060-piopwm.feat/reg_standard/tstat3.nii.gz`. Set again \"Min\" to 0 and \"Max\" to 5, and select the \"Red-Yellow\" colormap. Now, add the same file again (\"File\" > \"Add\"), but set \"Min\" to 0 and \"Max\" to -5, and use the \"Blue-Lightblue\" colormap. What you visualize here, like you did previously with the within-subject pattern estimate of the trial, is the pattern estimate of this *subject*. Given that we used the contrast \"active-passive\", the pattern here consists of both of voxels that are more active in the active condition than in the passive condition (i.e. the voxels > 0) *and* the voxels that are more active in the passive condition than in the active condition (i.e. the voxels < 0).   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, by creating design matrices for pattern extraction and looking at the results of the pattern-estimation procedure for within- and between-subject pattern analyses in fslview, you understand the differences between these two types better. So far, we demonstrated the main differences between how you would define GLM design matrices to extract beta/t-value patterns for within- and between-subject pattern analyses. Now, we're going to digress slightly (but for a good reason) and talk about object-oriented programming in Python (something you'll encounter a lot in the coming weeks!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Object-oriented programming\n",
    "Python is a so-called `object-oriented` language. What this means is that *everything in Python is some kind of 'object'*, each having it's own *class*. Understanding these concepts is one of the most difficult things in Python (or any other object-oriented language), but it is vital to understand how the language works, and how to use it effectively. Also, in the coming weeks, you use packages (such as [scikit-learn](http://scikit-learn.org)) which strongly rely on Python's object-oriented functionality. \n",
    "\n",
    "In this section, we'll walk you through the basics of objects and classes in Python.\n",
    "\n",
    "N.B.: if you already know about and understand the principles of object-oriented programming, skip this section by all means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.: classes vs. objects\n",
    "As stated, everything in Python is an object, which is of a certain class. In fact, in the Python tutorial/refresher, you've learned about a couple of different classes already. For example, there is the `string` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_string = 'this is an object of the string-class'\n",
    "print(type(my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you check the *class* of any object with the `type()` function. Let's check out some more types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj1 = [1, 2, 3]\n",
    "print(type(obj1))\n",
    "\n",
    "obj2 = {'entry1': [1, 2, 3], 'entry2': 'testtesttest'}\n",
    "print(type(obj2))\n",
    "\n",
    "obj3 = 5\n",
    "print(type(obj3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, you might be thinking: what is the difference between objects and classes? Well, one way to think about it is to ***think about the relationship between an object and a class as the relationship between a building and it's (architectural) plan.***\n",
    "\n",
    "In other words, just like a building plan outlines how a building should be constructed, does the class specify how an object should be created\\*. Classes in Python define, basically, the attributes (*i.e. things that an object **is/has***) and the methods (*i.e. what each object **can do***).\n",
    "\n",
    "\\* In programmer-lingo, an *object is always an **instance** of a class*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember (from the refresher) the distinction between methods and attributes? Let's look at (some of) the methods and attributes of a numpy-ndarray object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "my_array = np.array([1, 2, 3, 4])\n",
    "\n",
    "print('The class of my_array is: %s\\n' % type(my_array))\n",
    "\n",
    "# Let's checkout some attributes\n",
    "print(my_array.size)\n",
    "print(my_array.shape)\n",
    "print(my_array.ndim)\n",
    "\n",
    "# And some methods ...\n",
    "print(my_array.reshape((2, 2)))\n",
    "print(my_array.prod())\n",
    "print(my_array.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, attributes (like `size`, `shape`, and `ndim`) are things that describe **things that an object has/is/characterizes**, and methods (like `reshape()`, `prod()`, and `mean()`) are **things that an object can do**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Below, I imported some object with different types. Figure out the types. Which type has the particular method 'is_integer()'?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out the types; to which class belongs the method 'is_integer()'?\n",
    "from functions import return_some_objects\n",
    "obj1, obj2, obj3 = return_some_objects()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so let's reiterate what we know so far:\n",
    "\n",
    "**Classes (\"the plan\") tell Python how to create objects (\"the building\"). A major 'responsibility' of classes is to specify which attributes and methods each object has.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Custom classes\n",
    "Now, Python (and other modules, like Numpy) have a bunch of built-in classes (like `str`, `list`, `dict`, etc.), but it's also possible to create your own classes! Below, we'll outline how we'd create a custom `Person` class, which has certain attributes and methods. Bear with us, we'll explain how each elements in the class-definition works step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Person():\n",
    "    \"\"\" This `person` class is a (nonsense) example to\n",
    "    show how classes work. It takes the following parameters\n",
    "    when initialized:\n",
    "    \n",
    "    name : a string\n",
    "    age : an integer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    \n",
    "    def call_out_my_name(self):\n",
    "        \"This method calls out the name of the Person object. \"\"\"\n",
    "        print('My name is %s' % self.name)\n",
    "        \n",
    "    def update_age(self, number_to_add):\n",
    "        \"\"\" This method updates the age attribute by a certain number.\n",
    "        It takes one parameter, `number_to_add` (an integer). \"\"\"\n",
    "        self.age = self.age + number_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Alright, so now we defined a custom `Person`-class. As you can see, it looks very similar to when you define a function (but instead of using \"`def`\" you use the keyword \"`class`\").\n",
    "\n",
    "Remember, this is a \"plan\" (instruction) on how to create (\"initialize\") an object. Let's do this (i.e. initialize an object of class `Person`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_person_object = Person(name='Lukas', age=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the code-block above, we initialized a variable (or more specifically: the object) \"my_person_object\", which is an instance of the class `Person`. The way we initialized this object is how any object in Python is initialized\\*:\n",
    "![alt text](obj_class_diff.png)\n",
    "\n",
    "\\* \"Huh, if any object is initialized this way (`obj = Class(inputs)`) then why don't I see that when I initialize builtin Python objects like `strings`, `lists`, `dictionaries`, etc.\", you might righfully ask. Very good question, we'll get to that later!\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `my_person_object` has the methods and attributes that have been specified by the `Person` class! In other words, any method or attribute that you've defined in the class (\"the plan\") is now *accessible* in the initialized object `my_person_object` (\"the building\")!\n",
    "\n",
    "For example, as specified by the class, my_person_object now has the attributes `name` and `age`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(my_person_object.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(my_person_object.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it also has the methods `call_out_my_name()` and `update_age()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_person_object.call_out_my_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_to_update_age_with = 10\n",
    "my_person_object.update_age(number_to_update_age_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: create a new instance of the `Person` class (e.g. with your own name and age) and mess around with it a little bit (e.g. call the update_age() function a couple of times and see how the age-attribute changes). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mess around with a new Person instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, alright, when we instantiated the my_person_object object according to the Person class, somehow the class \"bound\" the attributes and methods to the object. \n",
    "\n",
    "*But how on earth does the class do that?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1: the \\_\\_init\\_\\_ method\n",
    "If you look at the class definition of `Person` above, you might recognize functions (started by the keyword `def`) inside the class, like `__init__()` and `call_out_my_name()`. These are methods! Just remember: any `def` *inside* a class is a method (as explained in the refresher, methods are just functions applied to objects themselves). \n",
    "\n",
    "Let's talk about the `__init__()` method, a daunting but necessary element of each (custom) class. While it looks complicated (with all the underscores everywhere), it does just one simple thing: **it binds attributes to the object it*self***. \n",
    "\n",
    "Basically, when you initialize an object (e.g. by `Person(name='some name', age=42)`), Python will secretly call the `__init__()` method. By convention, the first argument to `__init__()` is *always* \"self\". What \"self\" represents, is a bit tricky to understand, but try to see it like this: \n",
    "\n",
    "**\"self\" functions as a placeholder/template in the class, that will be 'filled in' by the specific object once it is initialized.**\n",
    "\n",
    "Alright, complicated stuff. Let's try to clarify this statement by looking at another (simpler) example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Computer():\n",
    "    # os = operating system\n",
    "    def __init__(self, os, owner):\n",
    "        \n",
    "        self.os = os\n",
    "        self.owner = owner\n",
    "        \n",
    "        # As a side-note, you don't have to give the attribute the same name as \n",
    "        # the parameter. For example, you could also do this:\n",
    "        #\n",
    "        # self.my_operating_system_whatevs = os\n",
    "        #\n",
    "        # However, it is customary to use the same name as the parameter for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "computer_object = Computer(os='Windows', owner='Noor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, a class is nothing more than an **instruction** (\"plan\") on how to create an object. As such, what you see in the Computer class definition, is an \"instruction\" that specifies that the attributes `os` and `owner` should be bound to the \"placeholder\" *self*. Once we *actually* initialize an object (as we did in the code cell above by `Computer(os='Windows', owner='Noor')`), the attributes ('Windows' and 'Noor') are not bound to the template *self* but to the actual `computer_object` object! That's why we can now access the `os` attribute from the `computer_object` itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(computer_object.os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \"self\" only functions as a placeholder in the class (the \"plan\"/\"instruction\"), which is replaced by the actual object upon initialization.\n",
    "\n",
    "Importantly, each argument that is listed in the `__init__()` method (here: `os` and `owner`) should be \"filled in\" when initializing a Computer object. Thus, for example, the following crashes because it expects and `owner` argument, but it hasn't received it during initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "another_computer = Computer(os='Linux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: In the error message above, you see that `__init__()` is called when we tried to initialize a Computer-object, just as we expected! But the error also says that it takes 3 argument, but only received 2. This might seem weird, since we only passed *one* argument (namely `os`='Linux'), but this in fact makes sense. Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's reiterate what we know of the `__init__()` method and its relation to classes so far:\n",
    "\n",
    "1. It binds attributes to the object itself;\n",
    "2. It always takes *self* as a first argument;\n",
    "3. *self* functions as a \"template\" for when an object is actually initialized;\n",
    "4. Any argument that is listed in the `__init__()` method will be expected upon initialization;\n",
    "5. Any attribute or method defined in the class will be accessible after initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2: other methods \n",
    "As discussed earlier, each function definition (recognizable by the `def` keyword) in a class refers to a method. Let's look at the `Person` class example again (copy-pasted from earlier in the document):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Person():\n",
    "    \"\"\" This `person` class is a (nonsense) example to\n",
    "    show how classes work. It takes the following parameters\n",
    "    when constructed:\n",
    "    \n",
    "    name : a string\n",
    "    age : an integer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    \n",
    "    def call_out_my_name(self):\n",
    "        \"This method calls out the name of the Person object. \"\"\"\n",
    "        print('My name is %s' % self.name)\n",
    "        \n",
    "    def update_age(self, number_to_add):\n",
    "        \"\"\" This method updates the age attribute by a certain number.\n",
    "        It takes one parameter, `number_to_add` (an integer). \"\"\"\n",
    "        self.age = self.age + number_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, apart from the `__init__()` method, there are two other methods: `call_out_my_name()` and `update_age()`. Like the `__init__()` method, they always expect *self* as the first argument. By always passing *self* to methods, we can access and use the attributes of the object. For example, the `call_out_my_name()` function accesses and uses the `name` attribute.\n",
    "\n",
    "Apart from *self*, methods can also take additional \"non-self\" arguments. This you see for example in the `update_age()` method: as any method, it takes *self* as the first argument, but also needs an additional argument - `number_to_add`. \n",
    "\n",
    "Thus, methods can use \"internal arguments\" (the class attributes) and \"external arguments\" (which you have to provide when you call the method).\n",
    "\n",
    "Then, inside the `update_age()` method, another interesting thing happens. We're actually modifying the `age` attribute. Whut?\n",
    "\n",
    "Let's look at what that entails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "personx = Person('Noor', 24) # Note that, like functions, you don't *need* to provide keywords, like name='Noor'\n",
    "print(\"This person, named %s, is %i years old\" % (personx.name, personx.age))\n",
    "\n",
    "# Let's now update Noor's age (let's assume that it's 20-04, Noor's birthday)\n",
    "personx.update_age(number_to_add=1)\n",
    "print(\"This person is now %i years old!\" % personx.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the `update_age()` method only modifies its own attributes, you don't need a `return` statement for the effect to take place. This construct of modifying internal attributes and thus omitting a return statement is also known as \"in-place modifications\". You can, however, both change (or even add) internal attributes *and* return things, like the following (more realistic) example shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "class Niftiloader():\n",
    "    # Tip: it is customary in Python to start custom classes with a capital letter!\n",
    "    \n",
    "    def __init__(self, path_to_nifti):\n",
    "        # Here, we bind the argument path_to_nifti to self\n",
    "        self.path_to_nifti = path_to_nifti\n",
    "        \n",
    "    def load_and_return_shape(self):\n",
    "        \n",
    "        nifti_file = nib.load(self.path_to_nifti)\n",
    "        self.loaded_data = nifti_file.get_data()\n",
    "        \n",
    "        nifti_shape = nifti_file.shape\n",
    "        \n",
    "        return nifti_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NiftiLoader class has a method `load_and_return_shape()` that both adds an attribute (namely `loaded_data`) *and* returns, additionally, the shape of the nifti-file (i.e. the voxel dimensions)! Let's first initialize a Niftiloader object and check which attributes it has after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_nifti = 'sub-0037_workingmemory_WITHIN.feat/stats/tstat1.nii.gz'\n",
    "\n",
    "# Here we initialize a Niftiloader object\n",
    "my_loader = Niftiloader(path_to_nifti=example_nifti)\n",
    "\n",
    "# path_to_nifti attribute exists!\n",
    "print(my_loader.path_to_nifti)\n",
    "\n",
    "# ... but loaded_data attributes doesn't (yet) ...\n",
    "print(my_loader.loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we call the `load_and_return_shape()` function (and return the loaded data!), suddenly the `loaded_data` attribute *does* exist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We call the method load_and_return_shape() here\n",
    "nif_shape = my_loader.load_and_return_shape()\n",
    "print(my_loader.loaded_data)\n",
    "\n",
    "# Also, we stored the shape that is returned from the load_and_return_shape() method into another variable ('nif_shape'),\n",
    "print('\\nThe shape of my nifti is: %s' % (nif_shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: time to practice! We copy-pasted the `Niftiloader` class below, and we'd like you to extend the class as follows: add a method (named `calculate_min_and_max()`) that only takes *self* as an input argument and gets both the minimum value and maximum value from the `loaded_data` attribute. Then, it should create two new attributes (`min` and `max`, which should contain these values). Lastly, it should also explicitly **return** the extracted min and max value as a single tuple.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "class Niftiloader():\n",
    "    \n",
    "    def __init__(self, path_to_nifti):\n",
    "        \n",
    "        self.path_to_nifti = path_to_nifti\n",
    "        \n",
    "    def load_and_return_shape(self):\n",
    "        \n",
    "        nifti_file = nib.load(self.path_to_nifti)\n",
    "        self.loaded_data = nifti_file.get_data()\n",
    "        \n",
    "        nifti_shape = nifti_file.shape\n",
    "        \n",
    "        return nifti_shape\n",
    "    \n",
    "    # Add the new method here!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can test your new Niftiloader object here!\n",
    "# It should not give any errors\n",
    "niftiloader_new = Niftiloader(path_to_nifti=example_nifti)\n",
    "\n",
    "# First call load_and_return_shape() to load in the data\n",
    "shape = niftiloader_new.load_and_return_shape()\n",
    "\n",
    "# And now we'll call your calculate_min_and_max() method!\n",
    "out = niftiloader_new.calculate_min_and_max()\n",
    "\n",
    "# Let's test if everything ran as expected\n",
    "# (We use the 'assert' statement to check if the ouput of the \n",
    "#  statement all return to True; if not, you made an error somewhere!)\n",
    "\n",
    "# Check if the 'min' and 'max' attributes exist!\n",
    "assert(all(hasattr(niftiloader_new, att) for att in ['min', 'max']))\n",
    "\n",
    "# Check if the return values are indeed a tuple of length 2\n",
    "assert(isinstance(out, tuple) and len(out) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: classes in builtin Python objects\n",
    "Remember that we told you that \"everything in Python is an object\"? And that each object is initialized according to a class, which takes the format: `obj = Someclass(args)`? You might have noticed that this format is not *always* used, most prominently in built-in Python classes, like `list`, `dict`, `string`, etc. \n",
    "\n",
    "Let's look at an example. We can initialize a list seemingly without a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_list_object = [1, 2, 3]\n",
    "print(type(my_list_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, what happened was that actually the `[]` functions as a \"shortcut\" for the class definition of a list! In fact, you can also just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just use the class definition list(input)\n",
    "my_list_object = list([1, 2, 3])\n",
    "my_list_object_with_brackets = [1, 2, 3]\n",
    "\n",
    "# Is it really the same?\n",
    "print(my_list_object == my_list_object_with_brackets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using [] to initialize a list is just a shortcut for the class definition `list()`. In fact, this is true for many builtin Python objects. \"Why?\", you ask? Well, it's simply because programmers are lazy beings. Builtin types (like list, string, dict, etc.) are used so often that they invented (even) shorter \"shortcuts\" to initialize those objects. \n",
    "\n",
    "Other \"shortcuts\" for builtin types are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To initialize a string\n",
    "str('my string') # the same as 'my string'\n",
    "dict(key1=[1, 2, 3], key2=[5, 2, 1]) # the same as {'key1': [1, 2, 3], 'key2': [5, 2, 1]}\n",
    "tuple((1, 2, 3)) # the same as (1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that should be enough to understand the object-oriented programming concepts we're going to encounter in the rest of this course! Now, we are going to work with some real (pattern-based) fMRI data and store it in a custom object!\n",
    "\n",
    "But not after a short assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 1** (3 points): write a custom class, named `NiftiExplorer`, that is initialized with one parameter: `path_to_nifti` (referring to a string pointing to the path of a 4D nifti file). Then, write three methods: the first one - `load_data()` should read in the Nifti-file (using nibabel) and create a new attribute: `data` (which should be the loaded data as a 4D numpy array). The second method, named `demean_data()`, should subtract the temporal mean from each voxel in the `data` attribute and store the result in the `data` attribute again (effectively \"updating\" it). A third method, named `plot_histogram_of_values()`. This method should take the `data` attribute and make a histogram of all its **non-zero** values using matplotlib (google how to do this if you forgot!). The method should take one parameter, `bins`, that is used in the matplotlib histogram function to control the number of bins. Hint: set the default range in the histogram to (-500, 500).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This makes sure that you plot inside the notebook\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement your class here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can run this code-block to see whether everything runs as expected\n",
    "# If there are no errors, you did everything correctly!\n",
    "\n",
    "nifti_path = 'filtered_func_data.nii.gz'\n",
    "nifex = NiftiExplorer(nifti_path)\n",
    "\n",
    "# check if it bound the param path_to_nifti to self\n",
    "assert(hasattr(nifex, 'path_to_nifti'))\n",
    "\n",
    "print('NiftiExplorer object is correctly initialized!')\n",
    "\n",
    "nifex.load_data()\n",
    "\n",
    "# Check if you created the attribute data\n",
    "assert(hasattr(nifex, 'data'))\n",
    "\n",
    "# and if it's indeed a numpy array\n",
    "assert(isinstance(nifex.data, np.ndarray))\n",
    "\n",
    "print('The load_data() method works correctly!')\n",
    "\n",
    "# Run demean_data\n",
    "nifex.demean_data()\n",
    "\n",
    "# quick-and-dirty way to test if every voxel's temporal mean is approx. 0\n",
    "np.testing.assert_almost_equal(nifex.data.mean(axis=3), 0, decimal=1)\n",
    "\n",
    "print('The demean_data() method works correctly!')\n",
    "\n",
    "# Check if plotting method works!\n",
    "nifex.plot_histogram_of_values(bins=100)\n",
    "\n",
    "print('The plot_histogram_of_values() method works correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Data representation\n",
    "In pattern analyses, there is a specific way to 'store' and represent brain patterns: as 2D matrices of shape N-samples \\* N-voxels. **Important**: often (and confusingly), people refer to voxels as (brain) 'features' in pattern analyses. So in articles people often refer to samples-by-features matrices!\n",
    "\n",
    "Anyway, this is how such a matrix looks like: \n",
    "![](data_representation.png)\n",
    "\n",
    "Each row thus represents the voxel pattern of a sample (or: \"instance\")!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: samples are defined differently for within- and between-subject designs, as discussed in the previous section. What constitutes (typically) a sample in between-subject designs? And in within-subject designs?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the originally 3D voxel patterns (e.g. whole-brain patterns of t-values) are flattened (also called \"vectorized\" or \"raveled\") such that we can stack all patterns vertically into a 2D matrix. There are two reasons why pattern analyses need this 2D format and thus discard spatial information about the voxel patterns:\n",
    " \n",
    "1. There are very few analyses that take spatial information into account in the first place (so by flattening we get rid of the spatial information); \n",
    "2. Most algorithms used \"under the hood\" by pattern analyses rely heavily on matrix algebra (which operate on 2D matrices by definition).\n",
    "\n",
    "Anyway, let's look at an example. We're going to work with the working-memory data (as outline in the beginning of the notebook). Suppose we want to investigate whether we can predict whether a trial is passive or active (factor: working memory load) from (whole-brain) voxel patterns. Consequently, this is a **within-subject design**. As such, we model each trial separately by fitting a single-trial design matrix to obtain patterns of t-values per trial (similar to the plot just before section **1.3**). The results are in the directory: `WITHIN/sub-0037_workingmemory_WITHIN.feat`. Check out the directory (again) and especially the stats-folder. You should see a bunch of nifti-files which contain 3D voxel patterns with either \"pe\" (parameter estimates, FSLs way of referring to $\\beta$s), \"cope\", \"varcope\", \"zstat\", or \"tstat\" values.\n",
    "\n",
    "For this analysis, we're going to use patterns of t-stats (as is generally recommended over $\\beta$s).<br>\n",
    "As you can see, there are 40 nifti-files with t-stats; these refer to the 40 trials in the experiment (32 active trials, 8 passive trials)! Given that we need to adhere to the data representation as outlined above, we are in the following situation:\n",
    "\n",
    "**What we have**: 40 (3D) nifti-files<br>\n",
    "**What we need**: one 2D numpy array of shape 40 x {whatever amount of voxels there are in those niftis}\n",
    "\n",
    "Alright, time to learn some Python gems that help us load in and transform those patterns into a usable 2D numpy matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: tips & tricks to load and transform (nifti-)files\n",
    "As a first thing, we need to find all the paths to the tstat nifti-files. Python has a nifty (pun intended) tool called \"`glob`\" which can find files/directories on disk using [wildcards](https://en.wikipedia.org/wiki/Wildcard_character). It is usually imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glob`, in Python, is a function that takes a path (as a string) with one or more wildcard characters (such as the `*`) and searches for files/directories on disk that match that. For example, let's try to find all the png-images in the current directory using glob (these are the images that I used inside this notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_search_string = '*.png'\n",
    "png_files = glob(my_search_string)\n",
    "print(png_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns a list with all the files/directories that matched the search-string. Note that you can also search files outside of the current directory (note that we're in the directory `/home/nipa_?/week_1` right now). To do so, we can simply specify the relative or absolute path to it. For example, suppose I would like to find all the home-directories of the students of this course. I could simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_student_home_dirs = glob('/home/nipa_*')\n",
    "print(all_student_home_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Now you have the skills to actually \"glob\" all the t-stats yourself! Use glob to find all the paths to the t-stats and store the results (a list with 40 strings) in a variable called `tstat_paths`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement the ToDo here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: `glob` returns unsorted paths (so in seemingly random order). It's better if we sort the paths before loading them in, so the order of the paths is more intuitive (the first file is tstat1, the seconds tstat2, etc.). Python has a builtin function `sorted()`, which takes a list and sorts it alphabetically. The problem, here, is that if we'd use that - i.e. `sorted(tstat_paths)` - it will actually sort the files as: tstat1, tstat10, tstat11, etc. See for yourself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sorted(tstat_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this issue, we wrote a little function (`sort_nifti_paths()`) that sorts the paths correctly. (If you're interested in how it works, check out the functions.py file in the week_1 directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's fix it\n",
    "from functions import sort_nifti_paths\n",
    "tstat_paths = sort_nifti_paths(tstat_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: in the code block below, write a loop that loads in the tstat nifti-files one by one (using nibabel) and store them in the already preallocated array \"X\". Note that \"X\" is a 2D matrix (samples-by-features), but each tstat-file contains a 3D array, so you need to \"flatten\" the 3D array to a single vector: use e.g. the numpy function \"flatten()\" or \"ravel()\". \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voxel_dims = (80, 80, 37)\n",
    "X = np.zeros((len(tstat_paths), np.prod(voxel_dims)))\n",
    "\n",
    "# Start your loop here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: You just globbed and loaded the within-subject patterns into a 2D samples-by-features data matrix, awesome! Now, do the same thing for the between-subject patterns (in the `BETWEEN/*.feat` directories). Use the patterns from the `act-pas` contrast (tstat3/zstat3) **from the reg_standard** directory. Glob the files and load them in using a similar for-loop as above!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We already defined the samples-by-features matrix for you\n",
    "voxel_dims = (91, 109, 91) # MNI152 2mm x/y/z voxel dimensions\n",
    "Xbetween = np.zeros((5, np.prod(voxel_dims)))\n",
    "\n",
    "# Glob the files here and then loop over them to fill the Xbetween matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to do the two ToDos (wow, that's a horrible sentence), then you've managed to do one of the most important things in pattern analyses: getting the data (patterns) in the right format: a 2D matrix of samples-by-(brain-)features. As you've seen, \"samples\" refer to \"trials\" (i.e. any instance of your feature-of-interest) in within-subject analyses while samples refer subjects in between-subject analyses. The (brain-)features represent single-trial contrasts against baseline in within-subject analyses and a 'condition-average' contrast against baseline (e.g. active > baseline) or against another condition (e.g. active > passive) in between-subject analyses.\n",
    "\n",
    "From here onwards, we will learn how to apply pattern analyses to data in this format, which is (in a way) *much* easier because we will use mostly existing packages (like [scikit-learn](http://scikit-learn.org) for decoding analyses). When you use the right tools, you can literally implement a pattern analysis is 30 lines of code. But that's for the next two weeks.\n",
    "\n",
    "For the final assignment of this week, you are going to create a custom class `Mvp` (which stands for **M**ulti**v**oxel **p**attern) that sort of \"organizes\" the loading process, data representation, and contains some methods that act upon (e.g. preprocess) the 2D samples-by-feature matrix. In the next weeks, you'll extend this Mvp-class with more functionality as you'll learn new things (e.g we'll add some decoding-related methods in week 2 and some RSA-related methods in week 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 2** (7 points). Create a new class, `Mvp`, that takes as input a list of paths to nifti-files that represent the samples in a given pattern analysis. This class should contain three methods:<br><br>\n",
    "1. a method `load()` that loads the nifti-files (which are by now an attribute) using a loop similar to the last ToDo; the resulting samples-by-features matrix should be bound to self as a new attribute \"`X`\";<br><br>\n",
    "2. another method, `standardize()`, that standardizes each feature (voxel) in the 2D `data` attribute. Standardization is a common \"preprocessing step\" in pattern analyses that ensures that feature (columns in the 2D matrix) has mean 0 and standard deviation 1 (more info [here](https://en.wikipedia.org/wiki/Feature_scaling#Standardization)). Essentially, you just subtract each column's mean from each value in the column and then divide each value in the column by the standard deviation in the column;<br><br>\n",
    "3. a method `apply_mask()`, that takes two parameters, `path_to_mask`, which should be a nifti-file containing a mask (e.g. a mask of the amygdala), and `threshold`, a number that should indicate the minimum value that a voxel in the mask should take to be included in the mask. This method should first load in the mask (using nibabel), then make a boolean array of the mask (array with True for voxels above the threshold, False for those below) and ravel (flatten) the result in a 1D vector, and finally use this boolean mask-array to index and update the columns of the `X` attribute such that afterwards it only contains voxels specified in the thresholded mask. <br><br>\n",
    "\n",
    "**Hints**:<br><br>\n",
    "1. Test your code! You can use the tstat_paths we \"globbed\" earlier from the `sub-0037_WITHIN.feat` directory; <br><br>\n",
    "2. The methods do not need to return anything: everything is applied to the internal attributes (mostly the `X` attribute); <br><br>\n",
    "3. We supplied a probabilistic amygdala mask for you (`sub-0037_workingmemory_WITHIN.feat/Left_Amygdala_mask_epi.nii.gz`) to test your `apply_mask()` method with. If your method works correctly, your `X` attribute should be of shape [40, 280] after applying the mask *with a threshold of 0* to it.<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define your class here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this block to test your class! If there are no errors, it's correct!\n",
    "\n",
    "# Filter the annoying divide-by-zero runtime error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We use the tstat_paths from earlier\n",
    "mvp = Mvp(tstat_paths)\n",
    "\n",
    "# Load the tstats \n",
    "mvp.load()\n",
    "\n",
    "# Check if the mvp has an attribute 'X'\n",
    "assert(hasattr(mvp, 'X'))\n",
    "\n",
    "# Check if 'X' is a numpy array\n",
    "assert(isinstance(mvp.X, np.ndarray))\n",
    "\n",
    "# Check if 'X' has the right samples-by-features shape\n",
    "assert(mvp.X.shape == (40, 80*80*37))\n",
    "\n",
    "print('The load() method is correct!')\n",
    "\n",
    "# This produces some NaNs (because we might divide by 0 for voxels outside the brain),\n",
    "# but that's fine\n",
    "mvp.standardize()\n",
    "\n",
    "# Check if all columns (i.e. features) have approx. mean 0 and std 1\n",
    "np.testing.assert_almost_equal(np.nansum(mvp.X.mean(axis=0)), 0)\n",
    "\n",
    "# calculate all non-NaN stds ...\n",
    "stds = np.nanstd(mvp.X, axis=0)[np.invert(np.isnan(np.nanstd(mvp.X, axis=0)))]\n",
    "\n",
    "# ... and check whether they're all 1 (by checking the sum)\n",
    "np.testing.assert_almost_equal(np.sum(stds), len(stds))\n",
    "\n",
    "print('The standardize() method is correct!')\n",
    "\n",
    "mvp.apply_mask('sub-0037_workingmemory_WITHIN.feat/Left_Amygdala_mask_epi.nii.gz', threshold=0)\n",
    "\n",
    "assert(mvp.X.shape == (40, 280))\n",
    "\n",
    "print('The apply_mask() method is correct!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
